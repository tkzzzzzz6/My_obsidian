## ABSTRACT
Spatially resolved multi-omics promises systems-level insight into cellular state, regulation, and communication, yet robustly integrating heterogeneous modalities while preserving spatial boundaries remains challenging. We present a lightweight framework that couples a \emph{near-identity} projection–residual–propagation encoder for stable, boundary-preserving representation learning with \emph{temperature-controlled} soft cross-graph alignment using a short warm-up detach to prevent early misalignment. To reconcile global topology with local refinement, we employ \emph{multi-scale} APPNP diffusion and a \emph{time-scheduled} global-to-local gate that progressively shifts emphasis from global coherence to boundary sharpening. A small MLP fuses modality-specific embeddings; graph decoders enforce modality faithfulness; a prototype-aware contrastive objective compacts clusters; and a mild EMA regularizer stabilizes learned feature graphs. The propagation cost is $O(E d)$ and we sparsify correspondences to mitigate $O(N^2)$ alignment overhead. Across three public benchmarks spanning distinct tissues and platforms, the proposed method achieves state-of-the-art performance on boundary-sensitive and information-theoretic clustering metrics, while maintaining favorable runtime and memory profiles. Ablations confirm the complementary roles of near-identity encoding, warm-up alignment, and scheduled multi-scale diffusion. These results highlight a simple, scalable path toward accurate and interpretable spatial multi-omics integration.
## Introduction
Spatial transcriptomics (ST) extends single-cell RNA sequencing (scRNA-seq) into the spatial domain by capturing transcripts \emph{in situ} while preserving the two-dimensional coordinates of their originating cells or spots~\cite{marx2021method,stahl2016visualization,rao2021exploring,moses2022museum}. In contrast to dissociation-based scRNA-seq, this preservation of spatial context enables direct mapping of gene-expression patterns within intact tissue architecture, typically at or near single-cell resolution (platform dependent)~\cite{asp2020spatially,wang2023advances}. By jointly retaining molecular readouts and spatial localization, ST reveals how heterogeneous cell populations are organized across anatomical boundaries and gradients, and how local microenvironments modulate cellular states through short-range cell--cell interactions. These spatially grounded measurements provide a principled basis for downstream analyses such as spatial domain delineation, cell-type mapping, and the inference of intercellular communication, laying the foundation for the integrative spatial multi-omics methods developed in this work.

Spatially resolved \emph{multi}-omics integrates transcript, chromatin, epigenetic, and protein layers within the same tissue to yield systems-level views of cellular state, regulation, and communication. These modalities are now measurable \emph{in situ} via scRNA-seq, ATAC-seq, CUT\&Tag, and multimodal platforms such as CITE-seq and Stereo-CITE-seq~\cite{zhu2021joint,Stoeckius2017large-scale,liao2023integrated,zhao2025Stereo-seqv2}. Integration, however, is challenging: modalities differ in dimensionality (e.g., ADT vs.\ RNA), carry modality-specific noise and chemistry-driven batch effects~\cite{Stuart2019comprehensive,Hao2021integrated}, and—in spatial settings—must align to anatomical coordinates while preserving local neighborhoods~\cite{marx2021method}. Moreover, single-cell and spatial datasets are sparse, heterogeneous, and non-i.i.d., unlike the large homogeneous corpora typical of NLP/CV~\cite{Lopez2018deep,Argelaguet2021computational}, limiting direct strategy transfer. These factors motivate methods for robust cross-modal alignment that jointly respect spatial geometry and modality-specific idiosyncrasies.

Despite rapid progress, integrative analysis of spatially resolved multi-omics remains nascent. Existing approaches typically fall into two categories: (i) methods that integrate multiple modalities but \emph{do not explicitly couple} them to spatial coordinates—such as MOFA+~\cite{Argelaguet2020mofa+}, TotalVI~\cite{gayoso2019totalvi}, MultiVI~\cite{Ashuach2023multivi}, CiteFuse~\cite{kim2020citefuse}, and PAST~\cite{Li2023past}—and (ii) spatial methods that model neighborhood structure but remain largely \emph{single-modality}, including STAGATE~\cite{Dong2022stagate} and DeepST~\cite{xu2022deepst}. More recently, a new wave of algorithms explicitly couples spatial and molecular information; for example, SpatialGlue~\cite{Long2024SpatialGlue} leverages graph neural networks with dual attention for modality-aware alignment, and PRAGA~\cite{huang2024pragaprototypeawaregraphadaptive} employs dynamic graph construction with probabilistic denoising for robust cross-modal representation learning. These advances underscore the promise of spatial multi-omics integration while highlighting the need for more generalizable frameworks that can accommodate heterogeneous noise and dimensional scales, transfer across platforms, and remain robust to complex tissue architectures.

Nevertheless, current approaches exhibit recurring limitations. Spatial structure and cross-modal coupling are often modeled separately or with fixed single-scale diffusion, leading to over-smoothing and loss of boundaries. Cross-modal alignment tends to rely on static anchors or dense similarities without stabilization (e.g., temperature control or warm-up), making it brittle under noise, modality imbalance, and chemistry-specific artifacts. Graph construction is typically fixed ($k$, metric), susceptible to hubness, and rarely adapted across tissues. Many models enforce a single shared latent space without explicit reconstruction, weakening modality faithfulness and interpretability. Finally, scalability ($O(N^2)$ correspondences) and robustness to batch/platform shifts, resolution mismatches, and partially observed modalities remain open challenges.