梯度对准改善了医疗图像的测试时间适应
# Abstract
Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task.
尽管近年来医学图像分割领域取得了显著进展，但来自不同中心的医学图像之间普遍存在的域偏移问题，阻碍了预训练模型的有效部署。为解决这一问题，许多测试时自适应（TTA）方法被提出，这些方法通过在推理过程中利用测试数据对预训练模型进行微调来发挥作用。然而，由于优化方向欠佳（由梯度决定）且步长固定（基于学习率），这些方法的优化效果往往不尽如人意。在本文中，我们提出了基于梯度对齐的测试时自适应（GraTa）方法，以改进优化过程中的梯度方向和学习率。与传统TTA方法主要优化源自自监督目标的伪梯度不同，我们的方法将辅助梯度与伪梯度相结合，以促进梯度对齐。这种梯度对齐使模型能够挖掘不同梯度之间的相似性，并校正梯度方向，使其接近与当前分割任务相关的经验梯度。此外，我们基于伪梯度和辅助梯度之间的余弦相似度设计了动态学习率，从而使预训练模型能够在多样的测试数据上进行自适应微调。大量实验证实了所提出的梯度对齐和动态学习率的有效性，并证明了我们的GraTa方法在基准医学图像分割任务上优于其他最先进的TTA方法。
# Introduction
Medical image segmentation assumes a pivotal role in computer-aided diagnosis, offering precise delineation of specific anatomical structures. Over the past years, considerable research efforts (Tajbakhsh et al. 2020; Salpea, Tzouveli, and Kollias 2022; Liu et al. 2020; Wang et al. 2022; Tang et al. 2023; Xu et al. 2024) have been devoted to medical image segmentation utilizing deep-learning techniques, resulting in significant progress. However, domain shift, primarily caused by variations in scanners, imaging protocols,and operators (Gibson et al. 2018; Ghafoorian et al. 2017), poses challenges for these models pre-trained on the labeled dataset (source domain) to generalize across different medical centers (target domain).

医学图像分割在计算机辅助诊断中发挥着至关重要的作用，能够精确地勾勒特定的解剖结构。近年来，大量研究（Tajbakhsh et al., 2020; Salpea et al., 2022; Liu et al., 2020; Wang et al., 2022; Tang et al., 2023; Xu et al., 2024）利用深度学习方法推动了医学图像分割的发展，并取得了显著进展。然而，由于扫描设备、成像协议和操作人员差异引起的**域偏移**（domain shift）（Gibson et al., 2018; Ghafoorian et al., 2017），使得基于有标注源域数据集训练的模型难以在不同医疗中心（目标域）上良好泛化。
Test-time adaptation (TTA) has emerged as a prospective paradigm to mitigate domain shift with minimal data demand, relying only on test data during inference. Several studies on TTA attempt to alleviate the domain shift by modifying the statistics stored in batch normalization (BN) layers (Wang et al. 2023b; Nado et al. 2020; Mirza et al. 2022; Zhang et al. 2023; Park et al. 2024). A typical example is DUA (Mirza et al. 2022), which incrementally adjusts the statistics within BN layers from the source to the target domain to enhance feature representations. Although these methods improve adaptation, their performance potential is limited since the model’s parameters remain fixed. The mainstream TTA methods focus on designing self-supervised objectives (e.g., entropy minimization, consistency constraint, and rotation prediction) to fine-tune the pre-trained models to boost their generalization performance on the target domain (Wang et al. 2021; Sun et al. 2020; Zhang et al. 2023; Niu et al. 2023; Nguyen et al. 2023; Sinha et al. 2023; Yang et al. 2022; Bateson, Lombaert, and Ben Ayed 2022; Wen et al. 2024). For instance, TENT (Wang et al. 2021) introduces an entropy minimization objective to TTA to optimize the affine-transformation parameters of pre-trained models, and TTT (Sun et al. 2020) updates the model parameters by predicting the rotation angle of test data in a self-supervised manner.

为缓解域偏移，**测试时自适应**（Test-time Adaptation, TTA）近年来成为一种具有前景的研究范式，它在推理阶段仅依赖测试数据即可进行模型适配。已有研究尝试通过修改批归一化（Batch Normalization, BN）层中的统计量来缓解域偏移（Wang et al., 2023b; Nado et al., 2020; Mirza et al., 2022; Zhang et al., 2023; Park et al., 2024）。例如，DUA（Mirza et al., 2022）逐步将 BN 层的统计量从源域调整至目标域，从而增强特征表示。尽管这些方法取得了一定改进，但由于模型参数保持固定，其性能提升仍有限。主流的 TTA 方法则通过设计**自监督任务**（如熵最小化、一致性约束、旋转预测等）来微调预训练模型，从而提升其在目标域上的泛化能力（Wang et al., 2021; Sun et al., 2020; Zhang et al., 2023; Niu et al., 2023; Nguyen et al., 2023; Sinha et al., 2023; Yang et al., 2022; Bateson et al., 2022; Wen et al., 2024）。例如，TENT（Wang et al., 2021）在 TTA 中引入熵最小化目标，优化 BN 层的仿射变换参数；而 TTT（Sun et al., 2020）则通过预测测试数据的旋转角度来自监督模型更新。
![[Pasted image 20250912212805.png]]

**Figure 1**: Illustration of our motivation. **(a)** We display a pseudo gradient $\nabla L_{pse}(\theta)$, which is primarily utilized for optimization but may diverge far from the empirical gradient $\nabla L_{emp}(\theta)$, tailored to the specific task (segmentation in this study). Existing methods typically optimize $\nabla L_{pse}(\theta)$ in a straightforward manner. **(b)** Our GraTa introduces an auxiliary gradient $\nabla L_{aux}(\theta)$ to minimize the angle between $\nabla L_{pse}(\theta)$ and $\nabla L_{aux}(\theta)$, resulting in a novel auxiliary objective, i.e., gradient alignment. **(c)** Although achieving complete alignment is challenging due to the different objectives of these two gradients, the model can learn to align their task-relevant components $\nabla_{\checkmark}$ through this auxiliary objective, approximating $\nabla L_{emp}(\theta)$ and facilitating effective fine-tuning. $\angle$ denotes the angle. 'Obj': Abbreviation of 'Objective'.
**图1**：我们的动机说明。**（a）** 我们展示了一个伪梯度∇Lₚₛₑ(θ)，它主要用于优化，但可能与特定任务（本研究中的分割）定制的经验梯度∇Lₑₘₚ(θ)相差甚远。现有方法通常以直接的方式优化∇Lₚₛₑ(θ)。**（b）** 我们的GraTa引入了一个辅助梯度∇Lₐᵤₓ(θ)，以最小化∇Lₚₛₑ(θ)和∇Lₐᵤₓ(θ)之间的角度，从而形成一个新的辅助目标，即梯度对齐。**（c）** 尽管由于这两个梯度的目标不同，实现完全对齐具有挑战性，但模型可以学习对齐它们与任务相关的组件

However, all these methods, based on gradient descent, overlook two critical elements in the optimization procedure: the **direction** and the **step-size**. The $i$-th optimization procedure of the pre-trained model with parameters $\theta$ can be formulated as $\theta_{i+1} \leftarrow \theta_i - \eta\nabla L_{pse}(\theta_i)$, where $\eta$ is the learning rate, and $\nabla L_{pse}$ denotes the gradient produced by the self-supervised objective function, called **pseudo gradient**. Obviously, the optimization direction is determined by $\nabla L_{pse}(\theta_i)$, and the optimization step-size depends on $\eta$. 

然而，这些基于梯度下降的方法在优化过程中忽视了两个关键要素：**方向**与**步长**。预训练模型在第 $i$ 次迭代的更新可表示为 $\theta_{i+1} \leftarrow \theta_i - \eta \nabla L_{\text{pse}}(\theta_i)$，其中 $\eta$ 为学习率，$\nabla L_{\text{pse}}$ 为由自监督目标生成的伪梯度。显然，优化方向由 $\nabla L_{\text{pse}}(\theta_i)$ 决定，而优化步长则取决于 $\eta$。然而，伪梯度与经验梯度（依赖标注数据，代表分割任务的真实方向）之间往往存在显著偏差，使得优化难以收敛到理想解。同时，几乎所有现有方法都采用固定学习率，限制了模型在多样化测试数据上的自适应能力。

In Figure 1(a), we displayed two gradients: an **empirical gradient**, customized for the specific task (segmentation in this study) using provided labels and not encountered in the TTA setup; and a **pseudo gradient**, primarily employed for fine-tuning pre-trained models in existing TTA methods. Ideally, the pseudo gradient should exhibit a direction similar to the empirical gradient. Unfortunately, due to the lack of reliable supervision, their directions may differ significantly, posing challenges to model optimization. Current TTA methods simply optimize the pseudo gradient, failing to address this issue. Moreover, concerning the learning rate, almost all these methods employ a fixed value, thereby restricting the pre-trained models from being adaptively fine-tuned on diverse test data.
在图1（a）中，我们展示了两种梯度：一种是**经验梯度**，它是为特定任务（本研究中的分割任务）定制的，使用了提供的标签，且在测试时自适应（TTA）设置中不会遇到；另一种是**伪梯度**，主要用于现有TTA方法中对预训练模型进行微调。理想情况下，伪梯度应表现出与经验梯度相似的方向。遗憾的是，由于缺乏可靠的监督，它们的方向可能存在显著差异，这给模型优化带来了挑战。当前的TTA方法只是对伪梯度进行优化，未能解决这一问题。此外，在学习率方面，几乎所有这些方法都采用固定值，这限制了预训练模型对不同测试数据进行自适应微调。

In this paper, we focus on optimizing both the optimization direction and the learning rate. We propose a novel TTA method, namely Gradient alignment-based Test-time adaptation (GraTa). GraTa aims to improve the gradient direction through gradient alignment and enables pre-trained models to adapt to test data using a dynamic learning rate. As shown in Figure 1(b), we incorporate a new auxiliary objective, i.e., gradient alignment, by introducing an auxiliary gradient to the pseudo one. Specifically, we employ the consistency loss to produce the pseudo gradient, while leveraging the gradient derived from the entropy loss as the auxiliary one. The entropy loss is computed on the original test data. The consistency loss is conducted on the weak and strong augmentation variants of the test data. Through alignment, the model is capable of excavating the similarities between distinct gradients, especially the components relevant to the current segmentation task, and approximating the empirical gradient (see Figure 1(c)). Furthermore, we also present a dynamic learning rate, which is inversely proportional to the angle between these two gradients, to adaptively fine-tune the pre-trained models. A larger angle implies greater conflict in the optimization of the two gradients, thus requiring a smaller learning rate, and vice versa. Extensive experiments demonstrate that our GraTa achieves a superior performance over other state-of-the-art TTA methods.
在本文中，我们重点对优化方向和学习率进行优化。我们提出了一种新颖的测试时自适应（TTA）方法，即基于梯度对齐的测试时自适应（GraTa）。GraTa旨在通过梯度对齐改善梯度方向，并使预训练模型能够利用动态学习率来适应测试数据。如图1（b）所示，我们通过向伪梯度引入辅助梯度，融入了一个新的辅助目标，即梯度对齐。具体而言，我们利用一致性损失来生成伪梯度，同时将由熵损失导出的梯度用作辅助梯度。熵损失是在原始测试数据上计算的。一致性损失则是在测试数据的弱增强和强增强变体上进行的。通过对齐，模型能够挖掘不同梯度之间的相似性，特别是与当前分割任务相关的成分，并逼近经验梯度（见图1（c））。此外，我们还提出了一种动态学习率，该学习率与这两个梯度之间的夹角成反比，用于自适应地微调预训练模型。夹角越大，意味着这两个梯度在优化过程中的冲突越大，因此需要更小的学习率，反之亦然。大量实验表明，我们的GraTa相较于其他最先进的TTA方法，取得了更优异的性能。
Our contributions are three-fold: (1) we rethink the optimization procedure in existing TTA methods and propose GraTa to improve both the optimization direction and stepsize; (2) we align the pseudo and auxiliary gradients, aimed at distinct objectives, to reduce the divergence of their taskspecific components, thereby improving the optimization direction; and (3) we design a variable learning rate considering the angle between distinct gradients, aiding in the dynamical determination of the optimization step-size for adaptive fine-tuning.
我们的贡献有三个方面：（1）我们重新思考了现有TTA方法中的优化过程，并提出了GraTa来改进优化方向和步长；（2）我们对齐了伪梯度和辅助梯度（二者针对不同目标），以减少其任务特定组件的分歧，从而改善优化方向；（3）我们设计了一种考虑不同梯度之间夹角的可变学习率，有助于动态确定自适应微调的优化步长。


# Related Work

# Method

# Experiments and Results

