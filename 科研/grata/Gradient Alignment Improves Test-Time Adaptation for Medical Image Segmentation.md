梯度对准改善了医疗图像的测试时间适应
# Abstract
Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task.
尽管近年来医学图像分割领域取得了显著进展，但来自不同中心的医学图像之间普遍存在的域偏移问题，阻碍了预训练模型的有效部署。为解决这一问题，许多测试时自适应（TTA）方法被提出，这些方法通过在推理过程中利用测试数据对预训练模型进行微调来发挥作用。然而，由于优化方向欠佳（由梯度决定）且步长固定（基于学习率），这些方法的优化效果往往不尽如人意。在本文中，我们提出了基于梯度对齐的测试时自适应（GraTa）方法，以改进优化过程中的梯度方向和学习率。与传统TTA方法主要优化源自自监督目标的伪梯度不同，我们的方法将辅助梯度与伪梯度相结合，以促进梯度对齐。这种梯度对齐使模型能够挖掘不同梯度之间的相似性，并校正梯度方向，使其接近与当前分割任务相关的经验梯度。此外，我们基于伪梯度和辅助梯度之间的余弦相似度设计了动态学习率，从而使预训练模型能够在多样的测试数据上进行自适应微调。大量实验证实了所提出的梯度对齐和动态学习率的有效性，并证明了我们的GraTa方法在基准医学图像分割任务上优于其他最先进的TTA方法。
# Introduction
Medical image segmentation assumes a pivotal role in computer-aided diagnosis, offering precise delineation of specific anatomical structures. Over the past years, considerable research efforts (Tajbakhsh et al. 2020; Salpea, Tzouveli, and Kollias 2022; Liu et al. 2020; Wang et al. 2022; Tang et al. 2023; Xu et al. 2024) have been devoted to medical image segmentation utilizing deep-learning techniques, resulting in significant progress. However, domain shift, primarily caused by variations in scanners, imaging protocols,and operators (Gibson et al. 2018; Ghafoorian et al. 2017), poses challenges for these models pre-trained on the labeled dataset (source domain) to generalize across different medical centers (target domain).

医学图像分割在计算机辅助诊断中发挥着至关重要的作用，能够精确地勾勒特定的解剖结构。近年来，大量研究（Tajbakhsh et al., 2020; Salpea et al., 2022; Liu et al., 2020; Wang et al., 2022; Tang et al., 2023; Xu et al., 2024）利用深度学习方法推动了医学图像分割的发展，并取得了显著进展。然而，由于扫描设备、成像协议和操作人员差异引起的**域偏移**（domain shift）（Gibson et al., 2018; Ghafoorian et al., 2017），使得基于有标注源域数据集训练的模型难以在不同医疗中心（目标域）上良好泛化。
Test-time adaptation (TTA) has emerged as a prospective paradigm to mitigate domain shift with minimal data demand, relying only on test data during inference. Several studies on TTA attempt to alleviate the domain shift by modifying the statistics stored in batch normalization (BN) layers (Wang et al. 2023b; Nado et al. 2020; Mirza et al. 2022; Zhang et al. 2023; Park et al. 2024). A typical example is DUA (Mirza et al. 2022), which incrementally adjusts the statistics within BN layers from the source to the target domain to enhance feature representations. Although these methods improve adaptation, their performance potential is limited since the model’s parameters remain fixed. The mainstream TTA methods focus on designing self-supervised objectives (e.g., entropy minimization, consistency constraint, and rotation prediction) to fine-tune the pre-trained models to boost their generalization performance on the target domain (Wang et al. 2021; Sun et al. 2020; Zhang et al. 2023; Niu et al. 2023; Nguyen et al. 2023; Sinha et al. 2023; Yang et al. 2022; Bateson, Lombaert, and Ben Ayed 2022; Wen et al. 2024). For instance, TENT (Wang et al. 2021) introduces an entropy minimization objective to TTA to optimize the affine-transformation parameters of pre-trained models, and TTT (Sun et al. 2020) updates the model parameters by predicting the rotation angle of test data in a self-supervised manner.

为缓解域偏移，**测试时自适应**（Test-time Adaptation, TTA）近年来成为一种具有前景的研究范式，它在推理阶段仅依赖测试数据即可进行模型适配。已有研究尝试通过修改批归一化（Batch Normalization, BN）层中的统计量来缓解域偏移（Wang et al., 2023b; Nado et al., 2020; Mirza et al., 2022; Zhang et al., 2023; Park et al., 2024）。例如，DUA（Mirza et al., 2022）逐步将 BN 层的统计量从源域调整至目标域，从而增强特征表示。尽管这些方法取得了一定改进，但由于模型参数保持固定，其性能提升仍有限。主流的 TTA 方法则通过设计**自监督任务**（如熵最小化、一致性约束、旋转预测等）来微调预训练模型，从而提升其在目标域上的泛化能力（Wang et al., 2021; Sun et al., 2020; Zhang et al., 2023; Niu et al., 2023; Nguyen et al., 2023; Sinha et al., 2023; Yang et al., 2022; Bateson et al., 2022; Wen et al., 2024）。例如，TENT（Wang et al., 2021）在 TTA 中引入熵最小化目标，优化 BN 层的仿射变换参数；而 TTT（Sun et al., 2020）则通过预测测试数据的旋转角度来自监督模型更新。
![[Pasted image 20250912212805.png]]

**Figure 1**: Illustration of our motivation. **(a)** We display a pseudo gradient $\nabla L_{pse}(\theta)$, which is primarily utilized for optimization but may diverge far from the empirical gradient $\nabla L_{emp}(\theta)$, tailored to the specific task (segmentation in this study). Existing methods typically optimize $\nabla L_{pse}(\theta)$ in a straightforward manner. **(b)** Our GraTa introduces an auxiliary gradient $\nabla L_{aux}(\theta)$ to minimize the angle between $\nabla L_{pse}(\theta)$ and $\nabla L_{aux}(\theta)$, resulting in a novel auxiliary objective, i.e., gradient alignment. **(c)** Although achieving complete alignment is challenging due to the different objectives of these two gradients, the model can learn to align their task-relevant components $\nabla_{\checkmark}$ through this auxiliary objective, approximating $\nabla L_{emp}(\theta)$ and facilitating effective fine-tuning. $\angle$ denotes the angle. 'Obj': Abbreviation of 'Objective'.
**图1**：我们的动机说明。**（a）** 我们展示了一个伪梯度∇Lₚₛₑ(θ)，它主要用于优化，但可能与特定任务（本研究中的分割）定制的经验梯度∇Lₑₘₚ(θ)相差甚远。现有方法通常以直接的方式优化∇Lₚₛₑ(θ)。**（b）** 我们的GraTa引入了一个辅助梯度∇Lₐᵤₓ(θ)，以最小化∇Lₚₛₑ(θ)和∇Lₐᵤₓ(θ)之间的角度，从而形成一个新的辅助目标，即梯度对齐。**（c）** 尽管由于这两个梯度的目标不同，实现完全对齐具有挑战性，但模型可以学习对齐它们与任务相关的组件

However, all these methods, based on gradient descent, overlook two critical elements in the optimization procedure: the **direction** and the **step-size**. The $i$-th optimization procedure of the pre-trained model with parameters $\theta$ can be formulated as $\theta_{i+1} \leftarrow \theta_i - \eta\nabla L_{pse}(\theta_i)$, where $\eta$ is the learning rate, and $\nabla L_{pse}$ denotes the gradient produced by the self-supervised objective function, called **pseudo gradient**. Obviously, the optimization direction is determined by $\nabla L_{pse}(\theta_i)$, and the optimization step-size depends on $\eta$. 

In Figure 1(a), we displayed two gradients: an **empirical gradient**, customized for the specific task (segmentation in this study) using provided labels and not encountered in the TTA setup; and a **pseudo gradient**, primarily employed for fine-tuning pre-trained models in existing TTA methods. Ideally, the pseudo gradient should exhibit a direction similar to the empirical gradient. Unfortunately, due to the lack of reliable supervision, their directions may differ significantly, posing challenges to model optimization. Current TTA methods simply optimize the pseudo gradient, failing to address this issue. Moreover, concerning the learning rate, almost all these methods employ a fixed value, thereby restricting the pre-trained models from being adaptively fine-tuned on diverse test data.

然而，这些基于梯度下降的方法在优化过程中忽视了两个关键要素：**方向**与**步长**。预训练模型在第 $i$ 次迭代的更新可表示为 $\theta_{i+1} \leftarrow \theta_i - \eta \nabla L_{\text{pse}}(\theta_i)$，其中 $\eta$ 为学习率，$\nabla L_{\text{pse}}$ 为由自监督目标生成的伪梯度。显然，优化方向由 $\nabla L_{\text{pse}}(\theta_i)$ 决定，而优化步长则取决于 $\eta$。然而，伪梯度与经验梯度（依赖标注数据，代表分割任务的真实方向）之间往往存在显著偏差，使得优化难以收敛到理想解。同时，几乎所有现有方法都采用固定学习率，限制了模型在多样化测试数据上的自适应能力。

为此，本文提出了一种新的 TTA 方法——**基于梯度对齐的测试时自适应（GraTa）**。GraTa 同时优化梯度方向与学习率：通过引入辅助梯度与伪梯度进行对齐，减少二者在任务相关分量上的差异，从而逼近经验梯度；并设计基于余弦相似度的动态学习率，以根据梯度间的角度自适应地调整步长。当伪梯度与辅助梯度冲突较大时，学习率被调小以避免不稳定更新；当二者方向一致时，则分配更大的学习率以加速收敛。大量实验证明，GraTa 在医学图像分割基准任务中显著优于现有最先进的 TTA 方法。

本文的主要贡献如下：

1. 我们重新审视现有 TTA 方法的优化过程，提出 GraTa 来改进优化方向与步长。
    
2. 通过对齐伪梯度与辅助梯度，减少其任务相关分量的偏差，从而改善优化方向。
    
3. 设计基于梯度余弦相似度的动态学习率，实现对优化步长的动态调整，促进更加稳健的测试时自适应。
    

---

要不要我帮你把 **另一篇 SPCL 融合 GIALR 的论文（ICASSP 模板那篇）** 的 Introduction 也翻译成中文，这样你就能对比两篇的写作风格？

# Related Work

# Method

# Experiments and Results

